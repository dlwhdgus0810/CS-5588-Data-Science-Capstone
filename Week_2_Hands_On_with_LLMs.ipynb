{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BAA_jm16NlJ3"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers accelerate datasets sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ODrRQk6DNy8z",
        "outputId": "2cc70262-d113-4c3c-ff88-90e9e3132662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda |CUDA: 12.6\n"
          ]
        }
      ],
      "source": [
        "import torch, time, json, os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Device:\", device, \"|CUDA:\", torch.version.cuda if torch.cuda.is_available() else \"N/A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JdTilbwSazD",
        "outputId": "416c2374-0629-4747-fc22-d892972c74c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
          ]
        }
      ],
      "source": [
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "alternative_model_id = \"microsoft/phi-2\"\n",
        "\n",
        "fallback_model_id = \"distilgpt2\"\n",
        "\n",
        "def load_model(model_name):\n",
        "  try:\n",
        "    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device_map=\"auto\" if device == \"cuda\" else None\n",
        "    )\n",
        "    return tok, mdl, model_name\n",
        "  except Exception as e:\n",
        "    print(f\"Primary model failed: {e} \\nFalling back to {fallback_model_id}...\")\n",
        "    tok = AutoTokenizer.from_pretrained(fallback_model_id, use_fast=True)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(\n",
        "        fallback_model_id,\n",
        "        dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device_map=\"auto\" if device == \"cuda\" else None\n",
        "    )\n",
        "    return tok, mdl, fallback_model_id\n",
        "\n",
        "tokenizer, model, active_model_id = load_model(model_id)\n",
        "print(\"Loaded model:\", active_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMumsR9-UT1c",
        "outputId": "d976a686-53f8-4aeb-e06f-bf758f931b26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explain what a Knowledge Graph is in healthcare, in 3 concise sentences.\n"
          ]
        }
      ],
      "source": [
        "gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    # device=0 if device == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "prompt = \"Explain what a Knowledge Graph is in healthcare, in 3 concise sentences.\"\n",
        "out = gen(\n",
        "    prompt,\n",
        "    max_new_tokens=120,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")[0][\"generated_text\"]\n",
        "\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn-hAu6ei33M"
      },
      "source": [
        "# [TinyLlama/TinyLlama-1.1B-Chat-v1.0]\n",
        "Explain what a Knowledge Graph is in healthcare, in 3 concise sentences.\n",
        "\n",
        "A Knowledge Graph is a sophisticated system that enables the integration of various sources of data from different sources into a single, comprehensive database of medical knowledge. It includes information from various sources such as medical journals, patient records, and other health-related websites. The system uses machine learning algorithms to identify patterns and relationships between entities, such as drugs, diseases, and symptoms, and provides users with relevant information and insights in a structured format.\n",
        "\n",
        "\n",
        "# [microsoft/phi-2]\n",
        "Explain what a Knowledge Graph is in healthcare, in 3 concise sentences.\n",
        "\n",
        "Solution:\n",
        "A Knowledge Graph is a representation of relationships between entities, such as medical conditions, drugs, and patients. It helps healthcare professionals to identify and analyze complex patterns in large datasets, leading to improved diagnosis and treatment.\n",
        "\n",
        "Follow-up Exercise 1:\n",
        "How is a Knowledge Graph different from a traditional relational database?\n",
        "\n",
        "Solution:\n",
        "A Knowledge Graph differs from a traditional relational database by representing entities as nodes and relationships as edges. It allows for more complex relationships between entities and enables the analysis of these relationships to extract insights that may not be apparent in a traditional database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Model Swap & Comparison\n",
        "\n",
        "TinyLlama’s answer is richer in descriptive detail and demonstrates stronger correctness than phi-2's answer.  \n",
        "In contrast, phi-2’s response is concise and well-formatted. However, it introduces itrrelevant content that was not part of the question.\n",
        "Overall, TinyLlama returned a better answer because it stayed focused on the prompt, provided more relevant details, and avoided unnecessary information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjVUpddOU-L2",
        "outputId": "dc50d307-33bc-45b1-9f37-5d2748ddece4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token count: 16\n",
            "First 20 token IDs: [1, 8218, 479, 17088, 3382, 1379, 508, 18195, 24609, 322, 19138, 675, 24899, 936, 11486, 29889]\n",
            "Back to text: <s> Large Language Models can draft emails and summarize clinical notes.\n"
          ]
        }
      ],
      "source": [
        "text = \"Large Language Models can draft emails and summarize clinical notes.\"\n",
        "ids = tokenizer(text).input_ids\n",
        "\n",
        "print(\"Token count:\", len(ids))\n",
        "print(\"First 20 token IDs:\", ids[:20])\n",
        "print(\"Back to text:\", tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z70R7CigOzh2",
        "outputId": "9f83ee2c-45ac-4e4f-f99d-93acbf056569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Variant 1 | temp=0.2 top_p=0.95 top_k=50 ---\n",
            "Give 3 shrot tips for writing reproducible data science code: 1. Use comments to explain what each line of code does. 2. Use indentation to make your code easier to read. 3. Use functions to encapsulate your code and make it easier to reuse. 4. Use variables to store data and keep track of your work. 5. Use error handling to catch unexpected errors and provide clear feedback to the user.\n",
            "(latency ~2.28s)\n",
            "\n",
            "--- Variant 2 | temp=0.8 top_p=0.9 top_k=50 ---\n",
            "Give 3 shrot tips for writing reproducible data science code: 1. Follow a consistent naming convention: Write your code using a consistent naming convention, such as PascalCase, CamelCase, or snake_case. 2. Minimize unnecessary variable names: Keep your code as minimal as possible by minimizing the number of variables and avoiding unnecessary variables. 3. Use descriptive variable names: Use descriptive variable names that clearly describe what the variable is doing. For example, instead of `data`, use `dataset`. 4\n",
            "(latency ~3.37s)\n",
            "\n",
            "--- Variant 3 | temp=1.1 top_p=0.85 top_k=50 ---\n",
            "Give 3 shrot tips for writing reproducible data science code:\n",
            "\n",
            "1. Avoid repetitive code.\n",
            "2. Use reusable libraries whenever possible.\n",
            "3. Write comments explaining what you're doing.\n",
            "\n",
            "By following these tips, you'll be able to write reproducible data science code that is easy to follow, modify, and reuse.\n",
            "(latency ~1.84s)\n"
          ]
        }
      ],
      "source": [
        "base_prompt = \"Give 3 shrot tips for writing reproducible data science code:\"\n",
        "\n",
        "settings = [\n",
        "    {\"temperature\": 0.2, \"top_p\": 0.95, \"top_k\": 50},\n",
        "    {\"temperature\": 0.8, \"top_p\": 0.90, \"top_k\": 50},\n",
        "    {\"temperature\": 1.1, \"top_p\": 0.85, \"top_k\": 50},\n",
        "]\n",
        "\n",
        "for i, s in enumerate(settings, 1):\n",
        "  t0 = time.time()\n",
        "  out = gen(\n",
        "      base_prompt,\n",
        "      max_new_tokens=100,\n",
        "      do_sample=True,\n",
        "      temperature=s[\"temperature\"],\n",
        "      top_p=s[\"top_p\"],\n",
        "      top_k=s[\"top_k\"],\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "  )[0][\"generated_text\"]\n",
        "  print(f\"\\n--- Variant {i} | temp={s['temperature']} top_p={s['top_p']} top_k={s['top_k']} ---\")\n",
        "  print(out)\n",
        "  print(f\"(latency ~{time.time()-t0:.2f}s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnGuP1oYkCAx"
      },
      "source": [
        "# 2. Decoding Parameters – Explain in Your Own Words\n",
        "\n",
        "Temperature controls randomness: low values (e.g., 0.2 in Variant 1) give safe, repetitive outputs, while high values (1.1 in Variant 3) create more diverse but less predictable text.\n",
        "Top-p sets a probability threshold: high top-p (0.95 in Variant 1) considers more token options, while lower top-p (0.85 in Variant 3) narrows choices to safer words.\n",
        "Top-k limits how many tokens are sampled: high k (50) allows more variety, low k forces the model to pick from fewer options.\n",
        "\n",
        "In the output above, Variant 1 with low temperature(0.2) generated simple and basic tips that can apply to any coding environment. Variant 2 gave more practical tips with examples, and Variant 3 was concisel, and it gave more creatvie answer such as using reusable libraries whenever possible.\n",
        "Use low temperature + higher top-p/k when you need accuracy and consistency (e.g., coding tips) and higher temperature + lower top-p/k for creativity or brainstorming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Hallucinations - Risks & Mitigations\n",
        "\n",
        "1. phi-2 Knowledge Graph Task: The model was asked to explain a Knowledge Graph in healthcare in three sentences. Instead, it added an unprompted follow-up exercise (“How is a Knowledge Graph different from a traditional relational database?”) along with a solution that was not requested.\n",
        "2. Reproducible Data Science Tips Task: The base prompt asked for three short tips. However, Variant 1 generated five tips instead of three, and Variant 2 listed four tips but cut off mid-sentence, failing to follow instructions.\n",
        "\n",
        "These hallucinations undermine reliability, especially in technical or academic contexts.  \n",
        "By grounding outputs with external sources and adjusting generation parameters for tighter control, we can reduce irrelevant additions and maintain closer adherence to the user’s intent.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFnN2SbZRIt0",
        "outputId": "f8c6b9ae-2cd2-4f21-986a-2954bd06c52b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transfer learning is a technique that allows a pre-trained deep learning model to be used for other tasks without having to train it from scratch. The pre-trained model has already learned from a large dataset, and it can be used to improve the performance of a new task.\n",
            "When fine-tuning small LLMs on tiny datasets, there are two main risks: (1) Overfitting: small datasets can make the LLM's training set too small, resulting in a model that is too sensitive to training data. (2) Inference slowdown: small datasets may not be large enough to train a large LLM on, which can result in inference slowdown. To mitigate these risks, some approaches include using larger training sets, fine-tuning on multiple LLMs, and using larger batch sizes.\n",
            "Certainly! LLMs are being used in various natural language processing applications,\n"
          ]
        }
      ],
      "source": [
        "def build_prompt(history, user_msg, system=\"Youu are a helpful data science assistant.\"):\n",
        "  convo = [f\"[SYSTEM] {system}\"]\n",
        "  for u, a in history[-3:]:\n",
        "    convo.append(f\"[USER] {u}\")\n",
        "    convo.append(f\"[ASSISTANT] {a}\")\n",
        "  convo.append(f\"[USER] {user_msg}\\n[ASSISTANT]\")\n",
        "  return \"\\n\".join(convo)\n",
        "\n",
        "history = []\n",
        "\n",
        "def chat_once(user_msg, max_new_tokens=128, temperature=0.7, top_p=0.9):\n",
        "  prompt = build_prompt(history, user_msg)\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "  with torch.no_grad():\n",
        "    t0 = time.time()\n",
        "    tokens = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "  text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "  reply = text.split(\"[ASSISTANT]\")[-1].strip()\n",
        "  history.append((user_msg, reply))\n",
        "  print(reply)\n",
        "\n",
        "# demo turns\n",
        "chat_once(\"In one sentence, what is transfer learning?\")\n",
        "chat_once(\"Name two risks when fine-tuning small LLMs on tiny datasets.\")\n",
        "chat_once(\"Suggest one mitigation for each risk.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "collapsed": true,
        "id": "8axcId3NRr6S",
        "outputId": "eafae28b-efd4-4272-9893-dc4bca0788f7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"One sentence: why eval metrics matter beyond accuracy.\",\n          \"Explain temperature vs. top-p to a PM.\",\n          \"Write a tweet (<=200 chars) about reproducible ML.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"One sentence: why eval metrics matter beyond accuracy.\",\n          \"Explain temperature vs. top-p to a PM.\",\n          \"Write a tweet (<=200 chars) about reproducible ML. Use a clear and concise format, and ensure that the tweet is optimized for engagement by including relevant hashtags, relevant links, and a call-to-action. Use a friendly and approachable tone, and aim to provide value to the audience by highlighting the benefits of reproducible ML.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latency_s\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.3889174441508993,\n        \"min\": 0.03,\n        \"max\": 2.84,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.04,\n          0.03,\n          1.82\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-491a0650-7e82-4ede-ab14-56903f0bf3a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>output</th>\n",
              "      <th>latency_s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Write a tweet (&lt;=200 chars) about reproducible...</td>\n",
              "      <td>Write a tweet (&lt;=200 chars) about reproducible...</td>\n",
              "      <td>1.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>One sentence: why eval metrics matter beyond a...</td>\n",
              "      <td>One sentence: why eval metrics matter beyond a...</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>List 3 checks before deploying a model to prod...</td>\n",
              "      <td>List 3 checks before deploying a model to prod...</td>\n",
              "      <td>2.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Explain temperature vs. top-p to a PM.</td>\n",
              "      <td>Explain temperature vs. top-p to a PM.</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-491a0650-7e82-4ede-ab14-56903f0bf3a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-491a0650-7e82-4ede-ab14-56903f0bf3a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-491a0650-7e82-4ede-ab14-56903f0bf3a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6d7043f3-d5db-4cf9-a3b2-9b145c2aadd4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6d7043f3-d5db-4cf9-a3b2-9b145c2aadd4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6d7043f3-d5db-4cf9-a3b2-9b145c2aadd4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_1e7bcfc9-281a-467f-b295-34c99c609bd9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1e7bcfc9-281a-467f-b295-34c99c609bd9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              prompt  \\\n",
              "0  Write a tweet (<=200 chars) about reproducible...   \n",
              "1  One sentence: why eval metrics matter beyond a...   \n",
              "2  List 3 checks before deploying a model to prod...   \n",
              "3             Explain temperature vs. top-p to a PM.   \n",
              "\n",
              "                                              output  latency_s  \n",
              "0  Write a tweet (<=200 chars) about reproducible...       1.82  \n",
              "1  One sentence: why eval metrics matter beyond a...       0.04  \n",
              "2  List 3 checks before deploying a model to prod...       2.84  \n",
              "3             Explain temperature vs. top-p to a PM.       0.03  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "prompts = [\n",
        "    \"Write a tweet (<=200 chars) about reproducible ML.\",\n",
        "    \"One sentence: why eval metrics matter beyond accuracy.\",\n",
        "    \"List 3 checks before deploying a model to production.\",\n",
        "    \"Explain temperature vs. top-p to a PM.\"\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for p in prompts:\n",
        "    t0 = time.time()\n",
        "    out = gen(p, max_new_tokens=100, do_sample=True, temperature=0.7, top_p=0.9,\n",
        "              pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"]\n",
        "    rows.append({\"prompt\": p, \"output\": out, \"latency_s\": round(time.time()-t0, 2)})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "D-MWhj-LbQC6",
        "outputId": "e9bd4e3d-bb69-45f2-badb-9bb15d7885a6"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_0ef6b45e-680b-4103-82e9-af87b2042f04\", \"outputs.csv\", 1173)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "df.to_csv(\"outputs.csv\", index=False, encoding=\"utf-8\")\n",
        "files.download(\"outputs.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
